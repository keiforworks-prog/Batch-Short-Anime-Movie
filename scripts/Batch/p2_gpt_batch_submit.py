#!/usr/bin/env python3
"""
Phase 2 Batch Submit: ç”»åƒç”Ÿæˆãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
prompts_data.jsonl ã‹ã‚‰ image_prompt ã‚’èª­ã¿è¾¼ã¿ã€OpenAI Batch API ã§é€ä¿¡

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
1. JSONLå½¢å¼ (prompts_data.jsonl) ã«å¯¾å¿œ
2. ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ1æšç›®ã¨æœ€å¾Œ2æšã¯é«˜å“è³ªç‰ˆï¼‰
3. Google Drive ã‹ã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
4. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ï¼ˆæ—¢å­˜ç”»åƒã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
5. ã‚³ã‚¹ãƒˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
"""
import os
import sys
import json
import glob
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from logger_utils import DualLogger
from project_utils import read_project_info, get_output_dir, ensure_image_output_dir
from config import (
    LOGS_DIR, GPT_IMAGE_MODEL, IMAGE_SIZE, IMAGE_QUALITY,
    TEST_MODE_LIMIT
)
from gdrive_checkpoint import check_drive_checkpoint, authenticate_gdrive, find_project_folder_on_drive

load_dotenv()

# ãƒ¢ãƒ‡ãƒ«åˆ¥ä¾¡æ ¼
MODEL_PRICES = {
    "gpt-image-1": 0.25,      # é«˜å“è³ªç‰ˆ
    "gpt-image-1-mini": 0.052 # Miniç‰ˆ
}


def select_model_for_image(index, total_count):
    """
    ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    
    Args:
        index: ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ1ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰
        total_count: ç·ç”»åƒæ•°
    
    Returns:
        tuple: (model_name, price_per_image)
    """
    # 1æšç›®ã¾ãŸã¯æœ€å¾Œ2æšã®å ´åˆã¯é«˜å“è³ªç‰ˆ
    if index == 1 or index >= total_count - 1:
        return "gpt-image-1", MODEL_PRICES["gpt-image-1"]
    else:
        return "gpt-image-1-mini", MODEL_PRICES["gpt-image-1-mini"]


def download_prompts_from_drive(project_name, output_file_path, logger):
    """
    Google Drive ã‹ã‚‰ prompts_data.jsonl ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    try:
        from googleapiclient.http import MediaIoBaseDownload
        from googleapiclient.discovery import build
        import io
        
        parent_folder_id = os.getenv("GDRIVE_PARENT_FOLDER_ID")
        if not parent_folder_id:
            return False
        
        creds = authenticate_gdrive()
        if not creds:
            return False
        
        service = build('drive', 'v3', credentials=creds)
        
        project_folder_id = find_project_folder_on_drive(service, project_name, parent_folder_id)
        if not project_folder_id:
            return False
        
        query = f"name='prompts_data.jsonl' and '{project_folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, spaces='drive', fields='files(id)').execute()
        files = results.get('files', [])
        
        if not files:
            logger.log(f"âš ï¸ Drive ã« prompts_data.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return False
        
        file_id = files[0]['id']
        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        
        done = False
        while not done:
            status, done = downloader.next_chunk()
        
        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
        with open(output_file_path, 'wb') as f:
            f.write(fh.getvalue())
        
        logger.log(f"â˜ï¸  Drive ã‹ã‚‰ prompts_data.jsonl ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        return True
    
    except Exception as e:
        logger.log(f"âš ï¸ Drive ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def load_prompts_from_jsonl(prompts_file_path, logger):
    """
    prompts_data.jsonl ã‚’èª­ã¿è¾¼ã¿ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
    """
    logger.log(f"ğŸ”„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {prompts_file_path}")
    
    if not os.path.exists(prompts_file_path):
        logger.log(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    
    try:
        prompts = []
        with open(prompts_file_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    if "index" not in data or "image_prompt" not in data:
                        logger.log(f"âš ï¸ è¡Œ{line_num}: å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        continue
                    prompts.append(data)
                except json.JSONDecodeError as e:
                    logger.log(f"âš ï¸ è¡Œ{line_num}: JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        logger.log(f"âœ… {len(prompts)} å€‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return prompts
    
    except Exception as e:
        logger.log(f"ğŸš¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []


def check_existing_images(image_output_dir, project_name, logger):
    """
    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½: ãƒ­ãƒ¼ã‚«ãƒ« â†’ Google Drive ã®é †ã§ç¢ºèª
    """
    existing_images = set()
    
    if os.path.exists(image_output_dir):
        try:
            local_images = glob.glob(os.path.join(image_output_dir, "*.png"))
            existing_images = set([os.path.basename(f) for f in local_images])
            
            if existing_images:
                logger.log(f"ğŸ”„ ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º: {len(existing_images)} æšã®ç”»åƒãŒæ—¢ã«ç”Ÿæˆæ¸ˆã¿")
                return existing_images
        except Exception as e:
            logger.log(f"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    
    # Google Drive ã‚’ç¢ºèª
    try:
        parent_folder_id = os.getenv("GDRIVE_PARENT_FOLDER_ID")
        if parent_folder_id:
            drive_images = check_drive_checkpoint(project_name, parent_folder_id, checkpoint_type="images")
            if drive_images:
                existing_images = set(drive_images)
                logger.log(f"â˜ï¸  Google Drive ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º: {len(existing_images)} æš")
    except Exception as e:
        logger.log(f"âš ï¸ Drive ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    
    return existing_images


def create_batch_file(prompts_to_process, total_count, project_folder, logger):
    """
    ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    
    Args:
        prompts_to_process: å‡¦ç†ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ
        total_count: ç·ç”»åƒæ•°ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠç”¨ï¼‰
        project_folder: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€
        logger: ãƒ­ã‚¬ãƒ¼
    
    Returns:
        str: ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    batch_requests = []
    
    for prompt_data in prompts_to_process:
        image_index = prompt_data["index"]
        image_prompt = prompt_data["image_prompt"]
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model, price = select_model_for_image(image_index, total_count)
        
        request = {
            "custom_id": f"image_{image_index:03d}",
            "method": "POST",
            "url": "/v1/images/generations",
            "body": {
                "model": model,
                "prompt": image_prompt,
                "size": IMAGE_SIZE,
                "quality": IMAGE_QUALITY,
                "output_format": "png"  # GPT Image ãƒ¢ãƒ‡ãƒ«ã¯ response_format ã§ã¯ãªã output_format ã‚’ä½¿ç”¨
            }
        }
        batch_requests.append(request)
        logger.log(f"  ğŸ“ ç”»åƒ {image_index}: {model} (${price}/æš)")
    
    # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    batch_file_path = os.path.join(project_folder, "gpt_batch_requests.jsonl")
    with open(batch_file_path, "w", encoding="utf-8") as f:
        for req in batch_requests:
            f.write(json.dumps(req, ensure_ascii=False) + "\n")
    
    logger.log(f"\nâœ… ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {batch_file_path}")
    logger.log(f"   ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {len(batch_requests)} ä»¶")
    
    return batch_file_path


def submit_batch_job(batch_file_path, project_folder, logger):
    """
    ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’é€ä¿¡
    """
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    
    logger.log("\nğŸ“¤ ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    with open(batch_file_path, "rb") as f:
        batch_input_file = client.files.create(
            file=f,
            purpose="batch"
        )
    
    logger.log(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {batch_input_file.id}")
    
    # ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’ä½œæˆ
    logger.log("ğŸ“¤ ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’é€ä¿¡ä¸­...")
    batch = client.batches.create(
        input_file_id=batch_input_file.id,
        endpoint="/v1/images/generations",
        completion_window="24h"
    )
    
    batch_id = batch.id
    logger.log(f"âœ… ãƒãƒƒãƒé€ä¿¡æˆåŠŸ: {batch_id}")
    
    # ãƒãƒƒãƒæƒ…å ±ã‚’ä¿å­˜
    batch_info = {
        "batch_id": batch_id,
        "input_file_id": batch_input_file.id,
        "submitted_at": datetime.now().isoformat(),
        "status": "validating",
        "batch_file_path": batch_file_path
    }
    
    batch_info_file = os.path.join(project_folder, "gpt_batch_info.json")
    with open(batch_info_file, "w", encoding="utf-8") as f:
        json.dump(batch_info, f, ensure_ascii=False, indent=2)
    
    logger.log(f"âœ… ãƒãƒƒãƒæƒ…å ±ã‚’ä¿å­˜: {batch_info_file}")
    
    return batch_id


def main():
    """ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼"""
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project_name, model_name, _ = read_project_info()
    if not project_name:
        print("ğŸš¨ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    output_dir = get_output_dir(project_name, model_name)
    image_output_dir = ensure_image_output_dir(project_name, model_name)
    prompts_file = os.path.join(output_dir, "prompts_data.jsonl")
    
    # ãƒ­ã‚°è¨­å®š
    os.makedirs(LOGS_DIR, exist_ok=True)
    log_file = os.path.join(LOGS_DIR, f"gpt_batch_submit_{project_name}.log")
    logger = DualLogger(log_file)
    
    try:
        logger.log(f"\n{'='*60}")
        logger.log(f"Phase 2-A (GPT Batch Submit): '{project_name}'")
        logger.log(f"{'='*60}")
        
        # API ã‚­ãƒ¼ç¢ºèª
        if not os.environ.get("OPENAI_API_KEY"):
            logger.log("ğŸš¨ ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            sys.exit(1)
        
        # prompts_data.jsonl ã‚’ç¢ºèª
        if not os.path.exists(prompts_file):
            logger.log("ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ã« prompts_data.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            logger.log("â˜ï¸  Google Drive ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã¾ã™...")
            
            if not download_prompts_from_drive(project_name, prompts_file, logger):
                logger.log("ğŸš¨ ã‚¨ãƒ©ãƒ¼: prompts_data.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                sys.exit(1)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿
        prompt_data_list = load_prompts_from_jsonl(prompts_file, logger)
        if not prompt_data_list:
            logger.log("ğŸš¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            sys.exit(1)
        
        total_count = len(prompt_data_list)
        logger.log(f"ğŸ“Š ç·ç”»åƒæ•°: {total_count} æš")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        existing_images = check_existing_images(image_output_dir, project_name, logger)
        
        # å…¨ã¦å®Œäº†ã—ã¦ã„ã‚‹å ´åˆ
        if len(existing_images) >= total_count:
            logger.log(f"\nâœ… å…¨ã¦ã®ç”»åƒãŒæ—¢ã«ç”Ÿæˆæ¸ˆã¿ã§ã™ ({len(existing_images)}/{total_count})")
            logger.log(f"â–¶ï¸  Phase 2 ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            sys.exit(0)
        
        # æœªå®Œäº†åˆ†ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        prompts_to_process = []
        for prompt_data in prompt_data_list:
            filename = f"{prompt_data['index']:03d}.png"
            if filename not in existing_images:
                prompts_to_process.append(prompt_data)
        
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å¯¾å¿œ
        if TEST_MODE_LIMIT > 0:
            logger.log(f"\nâš ï¸ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æœ€åˆã® {TEST_MODE_LIMIT} æšã®ã¿å‡¦ç†ã—ã¾ã™")
            prompts_to_process = prompts_to_process[:TEST_MODE_LIMIT]
        
        logger.log(f"\nğŸ“Š å‡¦ç†å¯¾è±¡: {len(prompts_to_process)} æš")
        logger.log(f"   æ—¢å­˜: {len(existing_images)} æš")
        logger.log(f"   ã‚¹ã‚­ãƒƒãƒ—: {total_count - len(prompts_to_process) - len(existing_images)} æš")
        
        if not prompts_to_process:
            logger.log("âœ… å‡¦ç†å¯¾è±¡ã®ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“")
            sys.exit(0)
        
        # ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        logger.log(f"\nğŸ“ ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆä¸­...")
        batch_file_path = create_batch_file(prompts_to_process, total_count, output_dir, logger)
        
        # ãƒãƒƒãƒé€ä¿¡
        batch_id = submit_batch_job(batch_file_path, output_dir, logger)
        
        # ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š
        high_quality_count = 0
        mini_count = 0
        for prompt_data in prompts_to_process:
            model, _ = select_model_for_image(prompt_data["index"], total_count)
            if model == "gpt-image-1":
                high_quality_count += 1
            else:
                mini_count += 1
        
        estimated_cost = (high_quality_count * MODEL_PRICES["gpt-image-1"] + 
                         mini_count * MODEL_PRICES["gpt-image-1-mini"])
        
        logger.log(f"\n{'='*60}")
        logger.log(f"âœ… ãƒãƒƒãƒé€ä¿¡å®Œäº†")
        logger.log(f"{'='*60}")
        logger.log(f"  ãƒãƒƒãƒID: {batch_id}")
        logger.log(f"  ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {len(prompts_to_process)} ä»¶")
        logger.log(f"  - é«˜å“è³ªç‰ˆ (gpt-image-1): {high_quality_count} æš")
        logger.log(f"  - Miniç‰ˆ (gpt-image-1-mini): {mini_count} æš")
        logger.log(f"  æ¨å®šã‚³ã‚¹ãƒˆ: ${estimated_cost:.2f}")
        logger.log(f"  å®Œäº†ã¾ã§æœ€å¤§24æ™‚é–“ã‹ã‹ã‚Šã¾ã™")
        logger.log(f"{'='*60}")
        
        # ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã«ç™»éŒ²ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
        try:
            from batch_crawler import register_batch
            register_batch(
                project_name=project_name,
                batch_id=batch_id,
                batch_type="gpt_images",
                output_dir=output_dir,
                model_name=model_name
            )
            logger.log(f"\nğŸ”„ ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã«ç™»éŒ²ã—ã¾ã—ãŸ")
            logger.log(f"   ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’èµ·å‹•: python batch_crawler.py start")
        except ImportError:
            logger.log(f"\nâš ï¸ batch_crawler ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰")
        except Exception as e:
            logger.log(f"\nâš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ç™»éŒ²ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰: {e}")
        
        # GCS ã«ã‚‚ç™»éŒ²ï¼ˆCloud Run ç”¨ï¼‰
        try:
            from google.cloud import storage
            import json
            
            gcs_bucket = os.environ.get("GCS_BUCKET_NAME")
            if gcs_bucket:
                client = storage.Client()
                bucket = client.bucket(gcs_bucket)
                blob = bucket.blob("batch_status.json")
                
                # æ—¢å­˜ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
                if blob.exists():
                    content = blob.download_as_text()
                    status_data = json.loads(content)
                else:
                    status_data = {"projects": {}}
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿½åŠ 
                from datetime import datetime
                status_data["projects"][project_name] = {
                    "batch_id": batch_id,
                    "batch_type": "gpt_images",
                    "status": "in_progress",
                    "submitted_at": datetime.now().isoformat(),
                    "output_dir": output_dir,
                    "model_name": model_name
                }
                
                # ä¿å­˜
                blob.upload_from_string(
                    json.dumps(status_data, ensure_ascii=False, indent=2),
                    content_type="application/json"
                )
                logger.log(f"â˜ï¸  GCS (Cloud Runç”¨) ã«ç™»éŒ²ã—ã¾ã—ãŸ")
        except ImportError:
            logger.log(f"\nâš ï¸ google-cloud-storage ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        except Exception as e:
            logger.log(f"\nâš ï¸ GCS ç™»éŒ²ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰: {e}")
        
        return True
        
    except Exception as e:
        logger.log(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        logger.log(traceback.format_exc())
        logger.save_on_error()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
