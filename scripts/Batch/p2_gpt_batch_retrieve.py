#!/usr/bin/env python3
"""
Phase 2 Batch Retrieve: ç”»åƒç”Ÿæˆãƒãƒƒãƒçµæœå–å¾—
OpenAI Batch API ã®çµæœã‚’å–å¾—ã—ã€ç”»åƒã‚’ä¿å­˜

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
1. ãƒãƒƒãƒã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ãƒãƒ¼ãƒªãƒ³ã‚°
2. çµæœã®å–å¾—ã¨ç”»åƒä¿å­˜
3. Google Drive ã¸ã®å³æ™‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
4. ã‚³ã‚¹ãƒˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
5. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ›´æ–°
"""
import os
import sys
import json
import time
import base64
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from logger_utils import DualLogger
from project_utils import read_project_info, get_output_dir, ensure_image_output_dir
from config import (
    LOGS_DIR, BATCH_CHECK_INTERVAL, BATCH_MAX_WAIT_TIME
)
from cost_tracker import CostTracker
from gdrive_checkpoint import authenticate_gdrive, find_project_folder_on_drive

load_dotenv()

# ãƒ¢ãƒ‡ãƒ«åˆ¥ä¾¡æ ¼
MODEL_PRICES = {
    "gpt-image-1": 0.25,
    "gpt-image-1-mini": 0.052
}


def load_batch_info(project_folder, logger):
    """
    ãƒãƒƒãƒæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    """
    batch_info_file = os.path.join(project_folder, "gpt_batch_info.json")
    
    if not os.path.exists(batch_info_file):
        logger.log(f"ğŸš¨ gpt_batch_info.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {batch_info_file}")
        return None
    
    with open(batch_info_file, "r", encoding="utf-8") as f:
        return json.load(f)


def check_batch_status(batch_id, logger):
    """
    ãƒãƒƒãƒã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
    """
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    batch = client.batches.retrieve(batch_id)
    
    # é€²æ—æƒ…å ±ã‚’è¡¨ç¤º
    if hasattr(batch, 'request_counts'):
        counts = batch.request_counts
        completed = getattr(counts, 'completed', 0)
        failed = getattr(counts, 'failed', 0)
        total = getattr(counts, 'total', 0)
        logger.log(f"  é€²æ—: {completed}/{total} å®Œäº†, {failed} å¤±æ•—")
    
    return batch


def upload_image_to_drive(image_path, project_name, logger):
    """
    ç”»åƒã‚’ Google Drive ã«å³åº§ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    try:
        from googleapiclient.http import MediaFileUpload
        from googleapiclient.discovery import build
        
        parent_folder_id = os.getenv("GDRIVE_PARENT_FOLDER_ID")
        if not parent_folder_id:
            return
        
        creds = authenticate_gdrive()
        if not creds:
            return
        
        service = build('drive', 'v3', credentials=creds)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢
        project_folder_id = find_project_folder_on_drive(service, project_name, parent_folder_id)
        
        if not project_folder_id:
            # ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„å ´åˆã¯ä½œæˆ
            folder_metadata = {
                'name': project_name,
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [parent_folder_id]
            }
            folder = service.files().create(body=folder_metadata, fields='id').execute()
            project_folder_id = folder.get('id')
        
        # images ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢ã¾ãŸã¯ä½œæˆ
        query = f"name='images' and '{project_folder_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
        results = service.files().list(q=query, spaces='drive', fields='files(id)').execute()
        files = results.get('files', [])
        
        if files:
            images_folder_id = files[0]['id']
        else:
            folder_metadata = {
                'name': 'images',
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [project_folder_id]
            }
            folder = service.files().create(body=folder_metadata, fields='id').execute()
            images_folder_id = folder.get('id')
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        filename = os.path.basename(image_path)
        query = f"name='{filename}' and '{images_folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, spaces='drive', fields='files(id)').execute()
        existing_files = results.get('files', [])
        
        if existing_files:
            file_id = existing_files[0]['id']
            media = MediaFileUpload(image_path, mimetype='image/png')
            service.files().update(fileId=file_id, media_body=media).execute()
        else:
            file_metadata = {'name': filename, 'parents': [images_folder_id]}
            media = MediaFileUpload(image_path, mimetype='image/png')
            service.files().create(body=file_metadata, media_body=media, fields='id').execute()
    
    except Exception as e:
        logger.log(f"âš ï¸ Drive ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰: {e}")


def retrieve_batch_results(batch_id, project_folder, project_name, image_output_dir, logger, tracker):
    """
    ãƒãƒƒãƒçµæœã‚’å–å¾—ã—ã¦ç”»åƒã‚’ä¿å­˜
    
    GPT Image ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ :
    {
        "response": {
            "status_code": 200,
            "body": {
                "created": 1234567890,
                "data": [
                    {
                        "b64_json": "base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒãƒ‡ãƒ¼ã‚¿"
                    }
                ]
            }
        }
    }
    """
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    
    logger.log(f"\nğŸ“¥ ãƒãƒƒãƒçµæœã‚’å–å¾—ä¸­: {batch_id}")
    
    # ãƒãƒƒãƒæƒ…å ±ã‚’å–å¾—
    batch = client.batches.retrieve(batch_id)
    
    if not batch.output_file_id:
        logger.log("ğŸš¨ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return 0, 0
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    logger.log(f"ğŸ“¥ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {batch.output_file_id}")
    file_response = client.files.content(batch.output_file_id)
    
    # çµæœã‚’å‡¦ç†
    results = []
    for line in file_response.text.strip().split('\n'):
        if line.strip():
            result = json.loads(line)
            results.append(result)
    
    logger.log(f"âœ… å–å¾—æˆåŠŸ: {len(results)} ä»¶")
    
    # ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’äº‹å‰ã«èª­ã¿è¾¼ã‚€
    model_map = {}  # custom_id -> model
    batch_info = load_batch_info(project_folder, logger)
    if batch_info and "batch_file_path" in batch_info:
        batch_file_path = batch_info["batch_file_path"]
        if os.path.exists(batch_file_path):
            with open(batch_file_path, "r", encoding="utf-8") as f:
                for line in f:
                    req = json.loads(line)
                    custom_id = req.get("custom_id", "")
                    model = req.get("body", {}).get("model", "")
                    model_map[custom_id] = model
    
    # ç”»åƒã‚’ä¿å­˜
    success_count = 0
    failed_count = 0
    high_quality_count = 0
    mini_count = 0
    
    for result in results:
        custom_id = None
        try:
            custom_id = result["custom_id"]
            image_num = int(custom_id.split("_")[1])
            
            response = result.get("response", {})
            status_code = response.get("status_code", 0)
            
            if status_code == 200:
                # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆGPT Image ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ï¼‰
                body = response.get("body", {})
                data_list = body.get("data", [])
                
                # b64_json ã‚’æ¢ã™
                b64_data = None
                if data_list:
                    # data[0] ã‹ã‚‰ b64_json ã‚’å–å¾—
                    b64_data = data_list[0].get("b64_json")
                
                if b64_data:
                    image_data = base64.b64decode(b64_data)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                    filename = f"{image_num:03d}.png"
                    filepath = os.path.join(image_output_dir, filename)
                    
                    with open(filepath, "wb") as f:
                        f.write(image_data)
                    
                    logger.log(f"âœ… ç”»åƒä¿å­˜: {filename}")
                    success_count += 1
                    
                    # ãƒ¢ãƒ‡ãƒ«åˆ¤å®šï¼ˆäº‹å‰ã«èª­ã¿è¾¼ã‚“ã ãƒãƒƒãƒ—ã‹ã‚‰å–å¾—ï¼‰
                    model = model_map.get(custom_id, "")
                    if model == "gpt-image-1":
                        high_quality_count += 1
                    else:
                        mini_count += 1
                    
                    # Google Drive ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                    upload_image_to_drive(filepath, project_name, logger)
                else:
                    logger.log(f"âš ï¸ ç”»åƒãƒ‡ãƒ¼ã‚¿ãªã—: {custom_id}")
                    logger.log(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {json.dumps(body, ensure_ascii=False)[:200]}...")
                    failed_count += 1
            else:
                error = response.get("error", {})
                error_msg = error.get("message", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
                logger.log(f"âš ï¸ å¤±æ•—: {custom_id} - {error_msg}")
                failed_count += 1
                
        except Exception as e:
            logger.log(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {custom_id} - {str(e)}")
            import traceback
            logger.log(traceback.format_exc())
            failed_count += 1
    
    # ã‚³ã‚¹ãƒˆè¨˜éŒ²
    tracker.add_phase_2(
        images_generated=success_count,
        images_failed=failed_count,
        images_high_quality=high_quality_count,
        images_mini=mini_count
    )
    
    # ãƒãƒƒãƒæƒ…å ±ã‚’æ›´æ–°
    batch_info_file = os.path.join(project_folder, "gpt_batch_info.json")
    if os.path.exists(batch_info_file):
        with open(batch_info_file, "r", encoding="utf-8") as f:
            batch_info = json.load(f)
        
        batch_info["status"] = "completed"
        batch_info["completed_at"] = datetime.now().isoformat()
        batch_info["success_count"] = success_count
        batch_info["failed_count"] = failed_count
        
        with open(batch_info_file, "w", encoding="utf-8") as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
    
    return success_count, failed_count


def main():
    """ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼"""
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project_name, model_name, _ = read_project_info()
    if not project_name:
        print("ğŸš¨ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    output_dir = get_output_dir(project_name, model_name)
    image_output_dir = ensure_image_output_dir(project_name, model_name)
    
    # ãƒ­ã‚°è¨­å®š
    os.makedirs(LOGS_DIR, exist_ok=True)
    log_file = os.path.join(LOGS_DIR, f"gpt_batch_retrieve_{project_name}.log")
    logger = DualLogger(log_file)
    
    # ã‚³ã‚¹ãƒˆãƒˆãƒ©ãƒƒã‚«ãƒ¼
    tracker = CostTracker(project_name)
    
    try:
        logger.log(f"\n{'='*60}")
        logger.log(f"Phase 2-B (GPT Batch Retrieve): '{project_name}'")
        logger.log(f"{'='*60}")
        
        # API ã‚­ãƒ¼ç¢ºèª
        if not os.environ.get("OPENAI_API_KEY"):
            logger.log("ğŸš¨ ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            sys.exit(1)
        
        # ãƒãƒƒãƒæƒ…å ±èª­ã¿è¾¼ã¿
        batch_info = load_batch_info(output_dir, logger)
        if not batch_info:
            sys.exit(1)
        
        batch_id = batch_info["batch_id"]
        logger.log(f"ğŸ“‹ ãƒãƒƒãƒID: {batch_id}")
        logger.log(f"ğŸ“‹ é€ä¿¡æ—¥æ™‚: {batch_info.get('submitted_at', 'ä¸æ˜')}")
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªãƒ«ãƒ¼ãƒ—
        start_time = time.time()
        check_count = 0
        
        while True:
            check_count += 1
            logger.log(f"\nğŸ”„ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª #{check_count}")
            
            batch = check_batch_status(batch_id, logger)
            status = batch.status
            
            logger.log(f"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
            
            if status == "completed":
                logger.log("\nâœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†!")
                break
            elif status in ["failed", "expired", "cancelled"]:
                logger.log(f"\nâŒ ãƒãƒƒãƒå¤±æ•—: {status}")
                if hasattr(batch, 'errors') and batch.errors:
                    logger.log(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {batch.errors}")
                logger.save_on_error()
                sys.exit(1)
            elif status == "in_progress":
                # é€²æ—è¡¨ç¤º
                pass
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            elapsed = time.time() - start_time
            if elapsed > BATCH_MAX_WAIT_TIME:
                logger.log(f"\nâŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({BATCH_MAX_WAIT_TIME}ç§’)")
                logger.save_on_error()
                sys.exit(1)
            
            # å¾…æ©Ÿ
            remaining = BATCH_MAX_WAIT_TIME - elapsed
            logger.log(f"  æ¬¡å›ãƒã‚§ãƒƒã‚¯ã¾ã§ {BATCH_CHECK_INTERVAL}ç§’å¾…æ©Ÿ...")
            logger.log(f"  æ®‹ã‚Šæ™‚é–“: {remaining/60:.1f}åˆ†")
            time.sleep(BATCH_CHECK_INTERVAL)
        
        # çµæœå–å¾—
        success_count, failed_count = retrieve_batch_results(
            batch_id, output_dir, project_name, image_output_dir, logger, tracker
        )
        
        # ã‚³ã‚¹ãƒˆã‚µãƒãƒªãƒ¼
        logger.log(f"\n{tracker.get_detailed_summary()}")
        
        # çµæœã‚µãƒãƒªãƒ¼
        logger.log(f"\n{'='*60}")
        logger.log(f"ğŸ“Š Phase 2-B å‡¦ç†çµæœ")
        logger.log(f"{'='*60}")
        logger.log(f"  - æˆåŠŸ: {success_count} æš")
        logger.log(f"  - å¤±æ•—: {failed_count} æš")
        logger.log(f"{'='*60}")
        
        if failed_count > 0:
            logger.log(f"âš ï¸ ä¸€éƒ¨ã®ç”»åƒç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚")
        else:
            logger.log(f"ğŸ‰ å…¨ã¦ã®ç”»åƒç”ŸæˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        
        logger.log("\n--- Phase 2-B (GPT Batch Retrieve) ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ ---")
        
        return True
        
    except Exception as e:
        logger.log(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        logger.log(traceback.format_exc())
        logger.save_on_error()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
