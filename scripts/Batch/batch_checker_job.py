#!/usr/bin/env python3
"""
Batch Checker Job: Cloud Scheduler ã‹ã‚‰å®šæœŸå®Ÿè¡Œã•ã‚Œã‚‹è»½é‡ã‚¸ãƒ§ãƒ–

ã€æ©Ÿèƒ½ã€‘
1. batch_status.json (Cloud Storage) ã‹ã‚‰ç›£è¦–å¯¾è±¡ã‚’å–å¾—
2. OpenAI API ã§ãƒãƒƒãƒã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
3. å®Œäº†ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚Œã° Post-flow Job ã‚’èµ·å‹•

ã€Cloud Scheduler è¨­å®šã€‘
- é »åº¦: */5 * * * * (5åˆ†ã”ã¨)
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: Cloud Run Job
"""
import os
import sys
import json
from datetime import datetime
from dotenv import load_dotenv

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from openai import OpenAI
from google.cloud import storage

load_dotenv()

# === è¨­å®š ===
GCS_BUCKET = os.environ.get("GCS_BUCKET_NAME", "")
BATCH_STATUS_BLOB = "batch_status.json"
PROJECT_ID = os.environ.get("GCP_PROJECT_ID", "")
REGION = os.environ.get("GCP_REGION", "asia-northeast1")
POST_FLOW_JOB_NAME = "batch-post-flow-job"


def load_batch_status_from_gcs():
    """
    Cloud Storage ã‹ã‚‰ batch_status.json ã‚’èª­ã¿è¾¼ã¿
    """
    if not GCS_BUCKET:
        print("âš ï¸ GCS_BUCKET_NAME ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return {"projects": {}}
    
    try:
        client = storage.Client()
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(BATCH_STATUS_BLOB)
        
        if not blob.exists():
            print("ğŸ“ batch_status.json ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return {"projects": {}}
        
        content = blob.download_as_text()
        return json.loads(content)
    
    except Exception as e:
        print(f"âš ï¸ GCS èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {"projects": {}}


def save_batch_status_to_gcs(status_data):
    """
    Cloud Storage ã« batch_status.json ã‚’ä¿å­˜
    """
    if not GCS_BUCKET:
        return
    
    try:
        client = storage.Client()
        bucket = client.bucket(GCS_BUCKET)
        blob = bucket.blob(BATCH_STATUS_BLOB)
        
        blob.upload_from_string(
            json.dumps(status_data, ensure_ascii=False, indent=2),
            content_type="application/json"
        )
        print(f"âœ… batch_status.json ã‚’ GCS ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    except Exception as e:
        print(f"âš ï¸ GCS ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


def check_batch_status_api(batch_id):
    """
    OpenAI API ã§ãƒãƒƒãƒã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
    """
    try:
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        batch = client.batches.retrieve(batch_id)
        
        # é€²æ—æƒ…å ±
        if hasattr(batch, 'request_counts'):
            counts = batch.request_counts
            completed = getattr(counts, 'completed', 0)
            failed = getattr(counts, 'failed', 0)
            total = getattr(counts, 'total', 0)
            print(f"  é€²æ—: {completed}/{total} å®Œäº†, {failed} å¤±æ•—")
        
        return batch.status, batch
    
    except Exception as e:
        print(f"âš ï¸ API ã‚¨ãƒ©ãƒ¼: {e}")
        return "error", None


def trigger_post_flow_job(project_name, project_info):
    """
    Cloud Run Job (Post-flow) ã‚’èµ·å‹•
    """
    try:
        from google.cloud import run_v2
        
        client = run_v2.JobsClient()
        
        # Job å
        job_name = f"projects/{PROJECT_ID}/locations/{REGION}/jobs/{POST_FLOW_JOB_NAME}"
        
        # ç’°å¢ƒå¤‰æ•°ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’æ¸¡ã™
        request = run_v2.RunJobRequest(
            name=job_name,
            overrides=run_v2.RunJobRequest.Overrides(
                container_overrides=[
                    run_v2.RunJobRequest.Overrides.ContainerOverride(
                        env=[
                            run_v2.EnvVar(name="TARGET_PROJECT_NAME", value=project_name),
                            run_v2.EnvVar(name="TARGET_BATCH_TYPE", value=project_info["batch_type"]),
                            run_v2.EnvVar(name="TARGET_OUTPUT_DIR", value=project_info["output_dir"]),
                            run_v2.EnvVar(name="TARGET_MODEL_NAME", value=project_info.get("model_name", "claude")),
                        ]
                    )
                ]
            )
        )
        
        operation = client.run_job(request=request)
        print(f"âœ… Post-flow Job ã‚’èµ·å‹•ã—ã¾ã—ãŸ: {project_name}")
        print(f"   Operation: {operation.operation.name}")
        
        return True
    
    except Exception as e:
        print(f"âŒ Post-flow Job ã®èµ·å‹•ã«å¤±æ•—: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print(f"\n{'='*60}")
    print(f"ğŸ” Batch Checker Job é–‹å§‹")
    print(f"   æ™‚åˆ»: {datetime.now().isoformat()}")
    print(f"{'='*60}")
    
    # API ã‚­ãƒ¼ç¢ºèª
    if not os.environ.get("OPENAI_API_KEY"):
        print("ğŸš¨ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)
    
    # batch_status.json ã‚’èª­ã¿è¾¼ã¿
    status_data = load_batch_status_from_gcs()
    projects = status_data.get("projects", {})
    
    if not projects:
        print("ğŸ“ ç›£è¦–å¯¾è±¡ã®ãƒãƒƒãƒãŒã‚ã‚Šã¾ã›ã‚“")
        print("âœ… Checker Job å®Œäº†")
        return
    
    print(f"ğŸ“‹ {len(projects)} ä»¶ã®ãƒãƒƒãƒã‚’ç¢ºèªä¸­...")
    
    completed_projects = []
    
    for project_name, project_info in projects.items():
        batch_id = project_info["batch_id"]
        current_status = project_info.get("status", "unknown")
        
        # æ—¢ã«å®Œäº†/å¤±æ•—ã—ã¦ã„ã‚‹ã‚‚ã®ã¯ã‚¹ã‚­ãƒƒãƒ—
        if current_status in ["completed", "failed", "expired", "cancelled", "post_flow_started"]:
            continue
        
        print(f"\nğŸ“‹ {project_name}")
        print(f"   Batch ID: {batch_id}")
        
        # API ã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        api_status, batch_obj = check_batch_status_api(batch_id)
        
        # çŠ¶æ…‹ã‚’æ›´æ–°
        project_info["status"] = api_status
        project_info["last_checked"] = datetime.now().isoformat()
        
        if api_status == "completed":
            print(f"âœ… ãƒãƒƒãƒå®Œäº†: {project_name}")
            completed_projects.append(project_name)
        
        elif api_status in ["failed", "expired", "cancelled"]:
            print(f"âŒ ãƒãƒƒãƒå¤±æ•—: {project_name} ({api_status})")
        
        else:
            print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {api_status}")
    
    # çŠ¶æ…‹ã‚’ä¿å­˜
    save_batch_status_to_gcs(status_data)
    
    # å®Œäº†ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã® Post-flow Job ã‚’èµ·å‹•
    for project_name in completed_projects:
        project_info = projects[project_name]
        
        if trigger_post_flow_job(project_name, project_info):
            # çŠ¶æ…‹ã‚’æ›´æ–°ï¼ˆé‡è¤‡èµ·å‹•é˜²æ­¢ï¼‰
            project_info["status"] = "post_flow_started"
            project_info["post_flow_started_at"] = datetime.now().isoformat()
    
    # æœ€çµ‚çŠ¶æ…‹ã‚’ä¿å­˜
    if completed_projects:
        save_batch_status_to_gcs(status_data)
    
    print(f"\n{'='*60}")
    print(f"âœ… Checker Job å®Œäº†")
    print(f"   å®Œäº†æ¤œçŸ¥: {len(completed_projects)} ä»¶")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
